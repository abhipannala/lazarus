This folder contains useful code shared by Harshit Mehta from his talk at Dell's Data Science Council on Kaggle techniques. Leaving this here for my reference.

I've used Bayesian Hyperparameter Optimization using bayesopt, need to switch to hyperopt, seems a more robust and it supports multiple optimization techniques!

Reduce memory usage is a neat trick to change the datatypes of columns to free up space on RAM. 

Adversarial validation seems like a fancy word for cheating :P If you are adding bias to your model to perform better on the test set's distribution, 
that is pretty much cheating. I can see how this works in a competition setting, but it would be bad in a production setting because you want the model to generalize. 

Feature selection using target permutation. I've read this before but haven't had a chance to apply/internalize it yet. One more addition to the learning bucket list!

Note: I should probably have a document that tracks my learnin to-do vs what I have completed
